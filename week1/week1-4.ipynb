{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 機械学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目次\n",
    "\n",
    "- k-最近傍法\n",
    "- ロジスティック回帰\n",
    "- 決定木\n",
    "- ランダムフォレスト\n",
    "- 勾配ブースティング\n",
    "- 非線形サポートベクターマシン\n",
    "- ハイパーパラメータチューニングチートシート"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section4 各アルゴリズムごとの実装例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Googleドライブのマウント（Colab使いのみ）\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# %cd /content/drive/MyDrive/dlc/week1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 図表が使えるようにする\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの分割\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# 訓練データのロード\n",
    "df_train = pd.read_csv('./data/train.csv')\n",
    "df_test = pd.read_csv('./data/test.csv')\n",
    "df_test_id = df_test['PassengerId']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理をする関数\n",
    "## train : pd.DataFrame \n",
    "## test : pd.DataFrame\n",
    "def preProcessing(train, test):\n",
    "    \n",
    "    # PassengerId, Name, Ticket, Cabinを削除する\n",
    "    train = train.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\n",
    "    test = test.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\n",
    "    \n",
    "    # Ageの欠損値を中央値で埋める\n",
    "    fill_age = train['Age'].median()\n",
    "    train['Age'] = train['Age'].fillna(fill_age)\n",
    "    test['Age'] = test['Age'].fillna(fill_age)\n",
    "    \n",
    "    # Fareの欠損値を平均値で埋める\n",
    "    fare_ave = train['Fare'].mean()\n",
    "    train['Fare'] = train['Fare'].fillna(fare_ave)\n",
    "    test['Fare'] = test['Fare'].fillna(fare_ave)\n",
    "\n",
    "    # Embarkedの欠損値を最頻値\"S\"で埋める\n",
    "    fill_embarked = 'S'\n",
    "    train['Embarked'] = train['Embarked'].fillna(fill_embarked)\n",
    "    test['Embarked'] = test['Embarked'].fillna(fill_embarked)\n",
    "    \n",
    "    # 家族の数(Family_size)という特徴量を作る\n",
    "    train['Family_size'] = train['SibSp'] + train['Parch'] + 1\n",
    "    test['Family_size'] = test['SibSp'] + test['Parch'] + 1\n",
    "\n",
    "    # SibSpとParchを削除する\n",
    "    train = train.drop(['SibSp', 'Parch'], axis=1)\n",
    "    test = test.drop(['SibSp', 'Parch'], axis=1)\n",
    "    \n",
    "    # カテゴリ変数をOne-Hotエンコーディング\n",
    "    train = pd.get_dummies(train, columns=['Sex', 'Pclass', 'Embarked'])\n",
    "    test = pd.get_dummies(test, columns=['Sex', 'Pclass', 'Embarked'])\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理\n",
    "train, test = preProcessing(df_train, df_test)\n",
    "\n",
    "# 特徴と正解ラベルに分ける\n",
    "feature = train.drop(['Survived'], axis=1)\n",
    "target = train['Survived']\n",
    "\n",
    "# 検証データ20%で分割する\n",
    "X_train, X_val, y_train, y_val = train_test_split(feature, target, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCVの結果をヒートマップで表示する関数\n",
    "def plotHeatmap(model):\n",
    "    \n",
    "    # チューニング対象のパラメータを特定する\n",
    "    params = [k for k in model.cv_results_.keys() if k.startswith('param_')]\n",
    "    \n",
    "    if len(params) != 2: \n",
    "        # ヒートマップの行、列、値に使うキーを定義する\n",
    "        index = ''\n",
    "        columns = params[0]\n",
    "        values = 'mean_test_score'\n",
    "\n",
    "        # gridから必要なキーのみを抽出する\n",
    "        df_dict = {k: model.cv_results_[k] for k in model.cv_results_.keys() & {index, columns, values}}\n",
    "\n",
    "        # dictをDataFrameに変換\n",
    "        df = pd.DataFrame(df_dict)\n",
    "        df[''] = ''\n",
    "        \n",
    "    else:\n",
    "        # ヒートマップの行、列、値に使うキーを定義する\n",
    "        index = params[0]\n",
    "        columns = params[1]\n",
    "        values = 'mean_test_score'\n",
    "\n",
    "        # gridから必要なキーのみを抽出する\n",
    "        df_dict = {k: model.cv_results_[k] for k in model.cv_results_.keys() & {index, columns, values}}\n",
    "\n",
    "        # dictをDataFrameに変換\n",
    "        df = pd.DataFrame(df_dict)\n",
    "    \n",
    "    data = df.pivot(index=index, columns=columns, values=values)\n",
    "    sns.heatmap(data, annot=True, fmt='.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# 混同行列を表示する関数\n",
    "def plotConfusionMatrix(model):\n",
    "    \n",
    "    best = model.best_estimator_\n",
    "    # 混同行列作成\n",
    "    pred_model = best.predict(X_val)\n",
    "    conf_matrix = confusion_matrix(y_val, pred_model)\n",
    "\n",
    "    # データフレーム作成\n",
    "    class_names = ['died', 'survived']\n",
    "    df_knn_conf_matrix = pd.DataFrame(conf_matrix,index=class_names,columns=class_names)\n",
    "\n",
    "    # グラフ作成\n",
    "    sns.heatmap(df_knn_conf_matrix, annot=True, cbar=None, cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Class')\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.show()\n",
    "    \n",
    "    # レポートも表示\n",
    "    print(\"Accuracy score of Validation: {:.2f}\".format(best.score(X_val, y_val)))\n",
    "    print(classification_report(y_val, pred_model, target_names=['died', 'survived']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 k-最近傍法 (K Nearest Neighbor)\n",
    "\n",
    "**メリット**\n",
    "- モデルが理解しやすい\n",
    "- あまり調整しなくても性能が出やすい\n",
    "- モデル構築が高速\n",
    "\n",
    "**デメリット**\n",
    "- 訓練セットが大きくなると予測が遅くなる\n",
    "  - 実際に使う時には前処理必須\n",
    "- 疎（特徴量の多くが0）なデータセットに対しては十分な性能が出にくい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knnを学習させてみる\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_params = {'n_neighbors': [1, 3, 5, 7, 9, 11]}\n",
    "knn_model = \\\n",
    "  GridSearchCV(\n",
    "    KNeighborsClassifier()\n",
    "    , knn_params\n",
    "    , cv=5\n",
    "    , return_train_score=True\n",
    "  )\n",
    "knn_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotHeatmap(knn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotConfusionMatrix(knn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 ロジスティック回帰 (Logistic Regression)\n",
    "\n",
    "**メリット**\n",
    "- 訓練、予測ともに高速\n",
    "- 大きなデータセット○\n",
    "- 疎なデータセット○\n",
    "\n",
    "\n",
    "**デメリット**\n",
    "- 収束しないことがある\n",
    "- 予測手法は理解しやすい反面、係数がなぜその値になっているのかは必ずしも自明ではない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_params = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "lr_model =  \\\n",
    "  GridSearchCV(\n",
    "    LogisticRegression(max_iter=1000)\n",
    "    , lr_params\n",
    "    , cv=5\n",
    "    , return_train_score=True\n",
    "  )\n",
    "lr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotHeatmap(lr_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotConfusionMatrix(lr_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 決定木 (Decision Tree)\n",
    "\n",
    "**メリット**\n",
    "- 可視化可能で、理解しやすい\n",
    "- ハイパーパラメータは3つのうちいずれか1つをいじるだけでいい\n",
    "\n",
    "**デメリット**\n",
    "- 過剰適合しやすい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtree_params = {\n",
    "    'max_depth':[1, 2, 3, 4, 5],\n",
    "    'min_samples_leaf':[1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'min_samples_split':[2, 3, 4, 5]\n",
    "}\n",
    "dtree_model = \\\n",
    "  GridSearchCV(\n",
    "    DecisionTreeClassifier()\n",
    "    , dtree_params\n",
    "    , cv=5\n",
    "    , return_train_score=True\n",
    "  )\n",
    "dtree_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best cross-validation score: {:.2f}\".format(dtree_model.best_score_))\n",
    "print(\"Best parameters: {}\".format(dtree_model.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotConfusionMatrix(dtree_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 ランダムフォレスト (Random Forest)\n",
    "\n",
    "**メリット**\n",
    "- 強力\n",
    "- ハイパーパラメータチューニングをさほど必要としない\n",
    "- スケール変換が不要\n",
    "\n",
    "**デメリット**\n",
    "- 訓練にも予測にも時間がかかる\n",
    "- メモリを大量に使う\n",
    "- 疎なデータにはうまく機能しない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "randforest_params = {\n",
    "    'n_estimators' : [100, 500, 1000],\n",
    "    'criterion' : ['gini'],\n",
    "    'min_samples_split' : [3, 5, 10],\n",
    "    'max_depth' : [5, 10, 15],\n",
    "    'random_state' : [1],\n",
    "    'verbose' : [False],\n",
    "}\n",
    "randforest_model = \\\n",
    "  GridSearchCV(\n",
    "    RandomForestClassifier()\n",
    "    , randforest_params\n",
    "    , cv=5\n",
    "    , return_train_score=True\n",
    "  )\n",
    "randforest_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best cross-validation score: {:.2f}\".format(randforest_model.best_score_))\n",
    "print(\"Best parameters: {}\".format(randforest_model.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotConfusionMatrix(randforest_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 勾配ブースティング (Gradient Boosting)\n",
    "\n",
    "**メリット**\n",
    "- かなり強力\n",
    "- スケール変換が不要\n",
    "\n",
    "**デメリット**\n",
    "- 訓練にも予測にも時間がかかる\n",
    "- メモリを大量に使う\n",
    "- 疎なデータにはうまく機能しない\n",
    "- ハイパーパラメータチューニングに気を使う必要がある"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_params = {'n_estimators': list(range(20,101,20)),\n",
    "             'learning_rate': [0.05, 0.10, 0.20]}\n",
    "gb_model = \\\n",
    "  GridSearchCV(\n",
    "    GradientBoostingClassifier(\n",
    "      min_samples_split = 5, \n",
    "      min_samples_leaf = 50, \n",
    "      max_depth = 5, \n",
    "      max_features = 'sqrt', \n",
    "      subsample = 0.8)\n",
    "    , gb_params\n",
    "    , cv=5\n",
    "    , return_train_score=True\n",
    "  )\n",
    "gb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotHeatmap(gb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotConfusionMatrix(gb_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 非線形サポートベクターマシン (Non-linear SVM)\n",
    "\n",
    "**メリット**\n",
    "- 強力\n",
    "- 特徴量が少なくても○\n",
    "\n",
    "**デメリット**\n",
    "- 訓練にも予測にも時間がかかる\n",
    "- メモリを大量に使う\n",
    "- データの前処理とハイパーパラメータチューニングが必須"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "SVC_params = {'C': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "              ,'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "SVC_model = \\\n",
    "  GridSearchCV(\n",
    "    SVC()\n",
    "    , SVC_params\n",
    "    , cv=5\n",
    "    , return_train_score=True\n",
    "  )\n",
    "SVC_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotHeatmap(SVC_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotConfusionMatrix(SVC_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 ハイパーパラメータチューニングチートシート\n",
    "\n",
    "|←適合不足|アルゴリズム名|過剰適合→|\n",
    "|:--|:-:|--:|\n",
    "|n_neighbors↑|k-最近傍法|n_neighbors↓|\n",
    "|C↓|ロジスティック回帰|C↑|\n",
    "|C↓|線形SVM|C↑|\n",
    "|C↓, gamma↓|非線形SVM|C↑, gamma↑|\n",
    "|max_depth↓|決定木|max_depth↑|\n",
    "|alpha↑|Ridge回帰/Lasso回帰|alpha↓|\n",
    "\n",
    "- alphaは大きなパラメータを罰する\n",
    "- Cは誤判別を罰する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaggleに提出するデータを作成\n",
    "\n",
    "X_test = df_test\n",
    "\n",
    "# 予測\n",
    "results = model.predict(X_test)\n",
    "results = pd.Series(results, name = 'Survived')\n",
    "\n",
    "# データ作成\n",
    "submission = pd.concat([df_test_id, results], axis = 1)\n",
    "submission.to_csv('titanic_submission.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7cf671263d4c51f2d66b480f72e1e8058975d5cfc878dca772fd6bfe3ea757f9"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('colab': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
