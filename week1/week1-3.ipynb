{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 機械学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目次\n",
    "\n",
    "- テストデータと訓練データ\n",
    "- オーバーフィッティングとアンダーフィッティング\n",
    "- ハイパーパラメータの調整\n",
    "  - 交差検証\n",
    "  - グリッドサーチ\n",
    "- モデルの評価"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section3 学習と評価の方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Googleドライブのマウント（Colab使いのみ）\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# %cd /content/drive/MyDrive/dlc/week1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 図表が使えるようにする\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 テストデータと訓練データ\n",
    "\n",
    "機械学習モデルを学習させる場合、手元にあるデータを **訓練データ (Train data)** と、 **テストデータ (Test data)** に予め分割します。これをホールドアウト法と言います。\n",
    "\n",
    "訓練データはモデルが学習するため、テストデータはモデルが「どれくらい正しく予測できているか」を評価するためのデータです。\n",
    "\n",
    "学習する際は、テストデータが全く見えない状態で行う必要が有ります。逆に、テストデータの中にあるべき変数が訓練データに洩れてしまっていることを **リーク (Data leakage)** と言います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの分割\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# 訓練データのロード\n",
    "data = pd.read_csv('./data/train.csv')\n",
    "feature = data.drop(['Survived'], axis=1)\n",
    "target = data['Survived']\n",
    "\n",
    "# テストデータ30%で分割する\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature, target, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前処理をする際も、訓練データとテストデータは分けるようにしましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理をする関数\n",
    "## train : pd.DataFrame \n",
    "## test : pd.DataFrame\n",
    "def preProcessing(train, test):\n",
    "    \n",
    "    # PassengerId, Name, Ticket, Cabinを削除する\n",
    "    train = train.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\n",
    "    test = test.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\n",
    "    \n",
    "    # Ageの欠損値を中央値で埋める\n",
    "    fill_age = train['Age'].median()\n",
    "    train['Age'] = train['Age'].fillna(fill_age)\n",
    "    test['Age'] = test['Age'].fillna(fill_age)\n",
    "\n",
    "    # Embarkedの欠損値を最頻値\"S\"で埋める\n",
    "    fill_embarked = 'S'\n",
    "    train['Embarked'] = train['Embarked'].fillna(fill_embarked)\n",
    "    test['Embarked'] = test['Embarked'].fillna(fill_embarked)\n",
    "    \n",
    "    # 家族の数(Family_size)という特徴量を作る\n",
    "    train['Family_size'] = train['SibSp'] + train['Parch'] + 1\n",
    "    test['Family_size'] = test['SibSp'] + test['Parch'] + 1\n",
    "\n",
    "    # SibSpとParchを削除する\n",
    "    train = train.drop(['SibSp', 'Parch'], axis=1)\n",
    "    test = test.drop(['SibSp', 'Parch'], axis=1)\n",
    "    \n",
    "    # カテゴリ変数をOne-Hotエンコーディング\n",
    "    train = pd.get_dummies(train, columns=['Sex', 'Pclass', 'Embarked'])\n",
    "    test = pd.get_dummies(test, columns=['Sex', 'Pclass', 'Embarked'])\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理\n",
    "\n",
    "X_train1, X_test1 = preProcessing(X_train, X_test)\n",
    "y_train1, y_test1 = y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習させてみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-最近傍法 (knn)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn1 = KNeighborsClassifier(n_neighbors=1)\n",
    "knn1.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの予測正解率を確認\n",
    "\n",
    "y_pred = knn1.predict(X_test1)\n",
    "print(\"Test set score: {:.2f}\".format(knn1.score(X_test1, y_test1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 オーバーフィッティングとアンダーフィッティング\n",
    "\n",
    "モデルが訓練データに対してすら良い予測精度を示さないという状態は **アンダーフィッティング（適合不足/学習不足）** と呼ばれます。\n",
    "\n",
    "一方で、訓練データに対しては良い予測精度を示すにも関わらず、テストデータに対してのパフォーマンスが良くない場合はモデルが訓練データに **オーバーフィッティング (過剰適合/過学習)** していると言います。\n",
    "\n",
    "<img src=\"figures/sweetspot.png\" width=\"60%\">\n",
    "\n",
    "画像の出典：https://qiita.com/kotamatsuoka/items/1ccb41ca278e400b6197"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練データをもとにつくったモデルが、未知のデータに対して同じような予測精度を示すなら **汎化 (Generalize)** できているといいます。この汎化性能が機械学習に置いて最重要視すべき観点です。\n",
    "\n",
    "一般的に、モデルを複雑にするほど訓練データに適合して行きます。適合不足にも過剰適合にもならない適度な複雑さの時に汎化能力が最大になるのでそこを目指しましょう。\n",
    "\n",
    "アルゴリズムの動作を制御するためのパラーメータ値を **ハイパーパラメータ (Hyperparameter)** と言い、これを調整してモデルの複雑さなどを変更します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-最近傍法 (knn)のハイパーパラメータを変えてみる\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn2 = KNeighborsClassifier(n_neighbors=3)\n",
    "knn2.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの予測正解率を確認\n",
    "\n",
    "y_pred = knn2.predict(X_test1)\n",
    "print(\"Test set score: {:.2f}\".format(knn2.score(X_test1, y_test1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 ハイパーパラメータの調整\n",
    "\n",
    "前述のホールドアウト法ではデータセットを訓練用・テスト用に2分割しましたが、実際の開発時にはモデルの性能評価をより適切にするためにデータを3分割してモデルを評価することが一般的です。\n",
    "\n",
    "| 名称 | 目的 |\n",
    "| :-- | :-- |\n",
    "| 訓練データ (Train data) | モデルの学習に利用 |\n",
    "| 検証データ (Validation data) | ハイパーパラメータの調整に利用 |\n",
    "| テストデータ (Test data) | モデルの最終性能評価に利用 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータ20%で分割する\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(feature, target, test_size=0.2)\n",
    "\n",
    "# 前処理\n",
    "X_train2, X_test2 = preProcessing(X_train2, X_test2)\n",
    "y_train2, y_test2 = y_train2, y_test2\n",
    "\n",
    "# 検証データ20%(=80%x25%)で分割する\n",
    "X_train2, X_val2, y_train2, y_val2 = train_test_split(X_train2, y_train2, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1 交差検証\n",
    "\n",
    "**交差検証 (Cross validation)** とは汎化性能を評価する統計的手法です。最も一般的な手法に **K-分割交差検証 (K-fold cross validation)** があります。\n",
    "\n",
    "K-分割交差検証は、データを $k$ 個に分割し、モデルの訓練と評価を $k$ 回行います。得られた $k$ 個の評価値の平均値を最終的なモデルのスコアとして扱います。\n",
    "\n",
    "データの量が少ない場合に有効です。\n",
    "\n",
    "<img src=\"figures/cross_validation.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 交差検証をしてみる\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "## cv: 分割数(=k)\n",
    "res = cross_validate(knn2, X_train1, y_train1, cv=5,\n",
    "                     return_train_score=True)\n",
    "display(pd.DataFrame(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2 グリッドサーチ\n",
    "\n",
    "**グリッドサーチ (Grid search)** とは指定したパラメータの全ての組み合わせに対して学習を行い、もっとも良い精度を示したパラメータを採用する手法です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単純なグリッドサーチ\n",
    "\n",
    "best_score = 0\n",
    "for nn in [1, 3, 5, 7, 9, 11]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=nn)\n",
    "    knn.fit(X_train2, y_train2)\n",
    "    score = knn.score(X_val2, y_val2)\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_parameters = {'n_neighbors' : nn}\n",
    "\n",
    "print(\"Best score: {:.2f}\".format(best_score))\n",
    "print(\"Best parameters: {}\".format(best_parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 交差検証を用いたグリッドサーチ\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "knn_params = {'n_neighbors': [1, 3, 5, 7, 9, 11]}\n",
    "knn_grid_search = GridSearchCV(\n",
    "                    KNeighborsClassifier()\n",
    "                    , knn_params\n",
    "                    , cv=5\n",
    "                    , return_train_score=True\n",
    "                  )\n",
    "knn_grid_search.fit(X_train2, y_train2)\n",
    "\n",
    "print(\"Best cross-validation score: {:.2f}\".format(knn_grid_search.best_score_))\n",
    "print(\"Best parameters: {}\".format(knn_grid_search.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 モデルの評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単純な生存率を確認\n",
    "\n",
    "survive_rate = y_train2.sum()/len(y_train2)\n",
    "print(\"Survive rate: {}\".format(survive_rate))\n",
    "print(\"Base line accuracy: {}\".format(1 - survive_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2クラス分類の評価結果を表現する方法の一つに **混同行列 (Confusion matrix)** があります。\n",
    "<table>\n",
    "\t<tbody>\n",
    "\t\t<tr>\n",
    "\t\t\t<td colspan=\"2\" rowspan=\"2\"></td>\n",
    "\t\t\t<td colspan=\"2\">モデルによる予測</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>0</td>\n",
    "\t\t\t<td>1</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td rowspan=\"2\">正解ラベル</td>\n",
    "\t\t\t<td>0\n",
    "</td>\n",
    "\t\t\t<td>TN<br>(True Negative)</td>\n",
    "\t\t\t<td>FP<br>(False Positive)</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>1</td>\n",
    "\t\t\t<td>FN<br>(False Negative)</td>\n",
    "\t\t\t<td>TP<br>(True Positive)</td>\n",
    "\t\t</tr>\n",
    "\t</tbody>\n",
    "</table>\n",
    "\n",
    "このように正解ラベルと予測に関してデータを振り分けると、各指標が次のように表せます。\n",
    "\n",
    "**正解率 (Accuracy)** = $\\frac{TP+TN}{TP+FN+FP+TN}$\n",
    "\n",
    "**適合率 (Precision)** = $\\frac{TP}{TP+FP}$\n",
    "\n",
    "**再現率 (Recall)** = $\\frac{TP}{TP+FN}$\n",
    "\n",
    "**F-値 (F-measure)** = $\\frac{2}{1/\\mathrm{Precision}+1/\\mathrm{Recall}}$ = $\\frac{{2}\\times{Precision}\\times{Recall}}{{Precision+Recall}}$：適合率と再現率の調和平均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 交差検証を用いたグリッドサーチでできたモデルの予測結果を混同行列で表示\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# 混同行列作成\n",
    "pred_knn_grid_search = knn_grid_search.predict(X_test2)\n",
    "knn_conf_matrix = confusion_matrix(y_test2, pred_knn_grid_search)\n",
    "\n",
    "# データフレーム作成\n",
    "class_names = ['died', 'survived']\n",
    "df_knn_conf_matrix = pd.DataFrame(knn_conf_matrix,index=class_names,columns=class_names)\n",
    "\n",
    "# グラフ作成\n",
    "sns.heatmap(df_knn_conf_matrix, annot=True, cbar=None, cmap='Blues')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel(\"True Class\")\n",
    "plt.xlabel(\"Predicted Class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各指標を出力\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "print(\"accuracy score knn: {:.2f}\".format(accuracy_score(y_test2, pred_knn_grid_search)))\n",
    "print(\"precision score knn: {:.2f}\".format(precision_score(y_test2, pred_knn_grid_search)))\n",
    "print(\"recall score knn: {:.2f}\".format(recall_score(y_test2, pred_knn_grid_search)))\n",
    "print(\"f1 score knn: {:.2f}\".format(f1_score(y_test2, pred_knn_grid_search)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# レポートを出力\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "### supportは個数を意味する\n",
    "print(classification_report(y_test2, pred_knn_grid_search, target_names=['died', 'survived']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-learnにおける教師有り学習のレシピ\n",
    "\n",
    "#### データを準備\n",
    "\n",
    "\n",
    "#### 前処理\n",
    "\n",
    "#### 訓練データとテストデータへの分割\n",
    "\n",
    "```\n",
    "sklearn.model_selection.train_test_split(, random_state=)\n",
    "```\n",
    "\n",
    "#### モデルの指定\n",
    "\n",
    "```\n",
    "from sklearn.<モジュール名> import <モデル名>\n",
    "model = <モデル名>(ハイパーパラメータ)\n",
    "```\n",
    "\n",
    "#### モデルの学習\n",
    "\n",
    "```\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "#### モデルの予測結果出力\n",
    "\n",
    "```\n",
    "y_pred = model.predict(X_test)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7cf671263d4c51f2d66b480f72e1e8058975d5cfc878dca772fd6bfe3ea757f9"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('colab': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
